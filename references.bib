
@inproceedings{BDCATS,
  author    = {Patwary, Md. Mostofa Ali and Byna, Suren and Satish, Nadathur Rajagopalan and Sundaram, Narayanan and LukiÄ‡, Zarija and Roytershteyn, Vadim and Anderson, Michael J. and Yao, Yushu and Prabhat and Dubey, Pradeep},
  booktitle = {{SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}},
  title     = {{BD-CATS: big data clustering at trillion particle scale}},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {1-12},
  doi       = {10.1145/2807591.2807616}
}

@article{soumagne2021accelerating,
  author    = {Soumagne, Jerome and Henderson, Jordan and Chaarawi, Mohamad and Fortner, Neil and Breitenfeld, Scot and Lu, Songyu and Robinson, Dana and Pourmal, Elena and Lombardi, Johann},
  journal   = {IEEE Transactions on Parallel and Distributed Systems},
  number    = {4},
  pages     = {903--914},
  publisher = {IEEE},
  title     = {{Accelerating hdf5 i/o for exascale using daos}},
  volume    = {33},
  year      = {2021}
}

@inproceedings{wu2021archtm,
  author    = {Wu, Kai and Ren, Jie and Peng, Ivy and Li, Dong},
  booktitle = {19th $\{$USENIX$\}$ Conference on File and Storage Technologies ($\{$FAST$\}$ 21)},
  pages     = {141--153},
  title     = {{ArchTM: Architecture-Aware, High Performance Transaction for Persistent Memory}},
  year      = {2021}
}

@inproceedings{tatebe2022chfs,
  author    = {Tatebe, Osamu and Obata, Kazuki and Hiraga, Kohei and Ohtsuji, Hiroki},
  booktitle = {{International Conference on High Performance Computing in Asia-Pacific Region}},
  pages     = {115--124},
  title     = {{CHFS: Parallel Consistent Hashing File System for Node-local Persistent Memory}},
  year      = {2022}
}

@article{liu2017dudetm,
  author    = {Liu, Mengxing and Zhang, Mingxing and Chen, Kang and Qian, Xuehai and Wu, Yongwei and Zheng, Weimin and Ren, Jinglei},
  journal   = {ACM SIGPLAN Notices},
  number    = {4},
  pages     = {329--343},
  publisher = {ACM New York, NY, USA},
  title     = {{DudeTM: Building durable transactions with decoupling for persistent memory}},
  volume    = {52},
  year      = {2017}
}

@inproceedings{ramalhete2021efficient,
  author    = {Ramalhete, Pedro and Correia, Andreia and Felber, Pascal},
  booktitle = {{Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  pages     = {1--15},
  title     = {{Efficient algorithms for persistent transactional memory}},
  year      = {2021}
}

@techreport{moody2017unifyfs,
  title       = {Unifyfs: A distributed burst buffer file system-0.1. 0},
  author      = {Moody, Adam and Sikich, Danielle and Bass, Ned and Brim, Michael J and Stanavige, Cameron and Sim, Hyogi and Moore, Joseph and Hutter, Tony and Boehm, Swen and Mohror, Kathryn and others},
  year        = {2017},
  institution = {Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States)}
}

@misc{vfdgds,
  howpublished = {\url{https://developer.nvidia.com/blog/gpudirect-storage/}},
  title        = {{GPUDirect Storage: A Direct Path Between Storage and GPU Memory}},
  year         = {2019}
}

@inproceedings{h5bench,
  author    = {Li, Tonglin and Byna, Suren and Koziol, Quincey and Tang, Houjun and Bez, Jean Luca and Kang, Qiao},
  booktitle = {{Proceedings of Cray User Group Meeting, CUG 2021}},
  title     = {{h5bench: HDF5 I/O Kernel Suite for Exercising HPC I/O Patterns}},
  year      = {2021}
}

@inproceedings{zheng2022hdf5,
  author       = {Zheng, Huihuo and Vishwanath, Venkatram and Koziol, Quincey and Tang, Houjun and Ravi, John and Mainzer, John and Byna, Suren},
  booktitle    = {{2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)}},
  organization = {IEEE},
  pages        = {61--70},
  title        = {{HDF5 Cache VOL: Efficient and Scalable Parallel I/O through Caching Data on Node-local Storage}},
  year         = {2022}
}

@software{julian_kunkel_2020_4391430,
  howpublished = {\url{https://github.com/hpc/ior}},
  title        = {{hpc/ior: IOR version 3.3.0}},
  year         = {2020}
}

@misc{lesbench,
  howpublished = {\url{https://github.com/tsukuba-ccs/les-io}},
  title        = {{LES-IO Benchmark}},
  year         = {2015}
}

@article{marathe2018persistent,
  author  = {Marathe, Virendra and Mishra, Achin and Trivedi, Amee and Huang, Yihe and Zaghloul, Faisal and Kashyap, Sanidhya and Seltzer, Margo and Harris, Tim and Byan, Steve and Bridge, Bill and others},
  journal = {arXiv preprint arXiv:1804.00701},
  title   = {{Persistent memory transactions}},
  year    = {2018}
}

@misc{ccs2022pegasus,
  howpublished = {\url{https://www.ccs.tsukuba.ac.jp/release221222e/}},
  title        = {{The University of Tsukuba announces plans for a New Big Memory Supercomputer ``Pegasus'' featuring upcoming 4th Gen Intel Xeon Scalable Processors, NVIDIA H100 PCIe GPU and Intel Optane Persistent Memory}},
  year         = {2022}
}

@article{tang2021transparent,
  author    = {Tang, Houjun and Koziol, Quincey and Ravi, John and Byna, Suren},
  journal   = {IEEE Transactions on Parallel and Distributed Systems},
  number    = {4},
  pages     = {891--902},
  publisher = {IEEE},
  title     = {{Transparent asynchronous parallel i/o using background threads}},
  volume    = {33},
  year      = {2021}
}

@misc{externalpassthrough,
  howpublished = {\url{https://github.com/hpc-io/vol-external-passthrough}},
  title        = {{vol-external-passthrough}},
  year         = {2020}
}

@article{byna2020exahdf5,
  title     = {{ExaHDF5: Delivering efficient parallel I/O on exascale computing systems}},
  author    = {Byna, Suren and Breitenfeld, M Scot and Dong, Bin and Koziol, Quincey and Pourmal, Elena and Robinson, Dana and Soumagne, Jerome and Tang, Houjun and Vishwanath, Venkatram and Warren, Richard},
  journal   = {Journal of Computer Science and Technology},
  volume    = {35},
  pages     = {145--160},
  year      = {2020},
  publisher = {Springer}
}

@inproceedings{pnetcdf,
  author    = {R. Latham and M. Zingale and R. Thakur and W. Gropp and B. Gallagher and W. Liao and A. Siegel and R. Ross and A. Choudhary and J. Li},
  booktitle = {{SC Conference}},
  title     = {{Parallel netCDF: A High-Performance Scientific I/O Interface}},
  year      = {2003},
  volume    = {},
  issn      = {},
  pages     = {39},
  abstract  = {Dataset storage, exchange, and access play a critical role in scientific applications. For such purposes netCDF serves as a portable, efficient file format and programming interface, which is popular in numerous scientific application domains. However, the original interface does not provide an efficient mechanism for parallel data storage and access. In this work, we present a new parallel interface for writing and reading netCDF datasets. This interface is derived with minimal changes from the serial netCDF interface but defines semantics for parallel access and is tailored for high performance. The underlying parallel I/O is achieved through MPI-IO, allowing for substantial performance gains through the use of collective I/O optimizations. We compare the implementation strategies and performance with HDF5. Our tests indicate programming convenience and significant I/O performance improvement with this parallel netCDF (PnetCDF) interface.},
  keywords  = {},
  doi       = {10.1109/SC.2003.10053},
  url       = {https://doi.ieeecomputersociety.org/10.1109/SC.2003.10053},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {nov}
}

@article{netcdf,
  author  = {Rew, R. and Davis, G.},
  journal = {IEEE Computer Graphics and Applications},
  title   = {{NetCDF: an interface for scientific data access}},
  year    = {1990},
  volume  = {10},
  number  = {4},
  pages   = {76-82},
  doi     = {10.1109/38.56302}
}

@inproceedings{hdf5,
  title     = {{HDF5: A file format and I/O library for high performance computing applications}},
  author    = {Folk, Mike and Cheng, Albert and Yates, Kim},
  booktitle = {{Proceedings of supercomputing}},
  volume    = {99},
  pages     = {5--33},
  year      = {1999}
}

@misc{lesbench,
  howpublished = {\url{https://github.com/tsukuba-ccs/les-io}},
  title        = {{LES-IO Benchmark}},
  year         = {2015}
}


@manual{Lustre,
  title = {Lustre: A Scalable, High-Performance File System},
  url   = {http://lustre.org}
}

@inproceedings{BurstFS,
  author    = {Wang, Teng and Mohror, Kathryn and Moody, Adam and Sato, Kento and Yu, Weikuan},
  title     = {{An Ephemeral Burst-buffer File System for Scientific Applications}},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  series    = {SC '16},
  year      = {2016},
  isbn      = {978-1-4673-8815-3},
  location  = {Salt Lake City, Utah},
  pages     = {69:1--69:12},
  articleno = {69},
  numpages  = {12},
  acmid     = {3014997}
}

@inproceedings{lang2009io,
  author    = {Lang, Samuel and Carns, Philip and Latham, Robert and Ross, Robert and Harms, Kevin and Allcock, William},
  booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
  title     = {I/O performance challenges at leadership scale},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {1-12},
  keywords  = {},
  doi       = {10.1145/1654059.1654100}
}

@inproceedings{wang2016burstbuffer,
  author    = {Wang, Teng and Mohror, Kathryn and Moody, Adam and Sato, Kento and Yu, Weikuan},
  booktitle = {SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  title     = {An Ephemeral Burst-Buffer File System for Scientific Applications},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {807-818},
  abstract  = {Burst buffers are becoming an indispensable hardware resource on large-scale supercomputers to buffer the bursty I/O from scientific applications. However, there is a lack of software support for burst buffers to be efficiently shared by applications within a batch-submitted job and recycled across different batch jobs. In addition, burst buffers need to cope with a variety of challenging I/O patterns from data-intensive scientific applications. In this study, we have designed an ephemeral Burst Buffer File System (BurstFS) that supports scalable and efficient aggregation of I/O bandwidth from burst buffers while having the same life cycle as a batch-submitted job. BurstFS features several techniques including scalable metadata indexing, co-located I/O delegation, and server-side read clustering and pipelining. Through extensive tuning and analysis, we have validated that BurstFS has accomplished our design objectives, with linear scalability in terms of aggregated I/O bandwidth for parallel writes and reads.},
  keywords  = {Buffer storage;Metadata;Indexing;Bandwidth;Pipeline processing;File systems;Distributed databases},
  doi       = {10.1109/SC.2016.68},
  issn      = {2167-4337},
  month     = {Nov}
}

@inproceedings{vef2018gekkofs,
  author    = {Vef, Marc-AndrÃ© and Moti, Nafiseh and SÃ¼ÃŸ, Tim and Tocci, Tommaso and Nou, Ramon and Miranda, Alberto and Cortes, Toni and Brinkmann, AndrÃ©},
  booktitle = {2018 IEEE International Conference on Cluster Computing (CLUSTER)},
  title     = {GekkoFS - A Temporary Distributed File System for HPC Applications},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {319-324},
  keywords  = {Metadata;Systems operation;Libraries;Servers;Semantics;Throughput;Data structures;Distributed File Systems;HPC;Burst Buffers},
  doi       = {10.1109/CLUSTER.2018.00049}
}

